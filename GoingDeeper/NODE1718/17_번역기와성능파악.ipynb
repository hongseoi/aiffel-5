{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815d8be7",
   "metadata": {},
   "source": [
    "# transformer로 번역기를 제작하고 성능을 평가해보자\n",
    "\n",
    "## 목차\n",
    "1. 번역 데이터 준비\n",
    "2. Transformer를 사용한 번역 모델 제작\n",
    "3. BLEU Score를 활용한 번역 성능 측정\n",
    "4. Beam Search Decoder를 활용한 번역 성능 측정\n",
    "5. 데이터 증강\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e146e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03e718",
   "metadata": {},
   "source": [
    "## 번역 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95ca11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 준비 완료\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "print('데이터 준비 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9355c4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 118964\n",
      ">> They arrived half an hour early.\tSe anticiparon media hora.\n",
      ">> Has anyone else seen this?\t¿Alguien más vio esto?\n",
      ">> Why are you angry with him?\t¿Por qué estás enfadado con él?\n",
      ">> I got up early to catch the first train.\tMe levanté temprano para coger el primer tren de la mañana.\n",
      ">> I hate myself and I want to die.\tMe odio a mí mismo y quiero morirme.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences)) \n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffaf0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 대문자를 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b34d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efac8d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  594\n",
      "\n",
      "\n",
      "Train Example: 118370\n",
      ">> they arrived half an hour early.\tse anticiparon media hora.\n",
      ">> has anyone else seen this?\t¿alguien más vio esto?\n",
      ">> why are you angry with him?\t¿por qué estás enfadado con él?\n",
      ">> i got up early to catch the first train.\tme levanté temprano para coger el primer tren de la mañana.\n",
      ">> i hate myself and i want to die.\tme odio a mí mismo y quiero morirme.\n",
      "\n",
      "\n",
      "Test Example: 594\n",
      ">> i had hoped that he would recover.\tyo esperaba que él se recuperaría.\n",
      ">> i explained the rules of the game to them.\tles expliqué las reglas del juego.\n",
      ">> i feel like a drink.\tme apetece beber algo.\n",
      ">> i'm looking for a sweater.\tbusco un suéter.\n",
      ">> i asked for his help.\tle pedí ayuda.\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 데이터 분리\n",
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n",
    "test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(train_spa_eng_sentences))\n",
    "for sen in train_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_spa_eng_sentences))\n",
    "for sen in test_spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1b6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77c2dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bee61f82bf4a2582b8361543add76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118370\n",
      "they arrived half an hour early.\n",
      "\n",
      "\n",
      "118370\n",
      "se anticiparon media hora.\n"
     ]
    }
   ],
   "source": [
    "train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n",
    "print(len(train_eng_sentences))\n",
    "print(train_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(train_spa_sentences))\n",
    "print(train_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a377ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7d67fd00b64d049ccbbd00bc19a025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "i had hoped that he would recover.\n",
      "\n",
      "\n",
      "594\n",
      "yo esperaba que él se recuperaría.\n"
     ]
    }
   ],
   "source": [
    "test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n",
    "print(len(test_eng_sentences))\n",
    "print(test_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(test_spa_sentences))\n",
    "print(test_spa_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfd40e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i had hoped that he would recover.',\n",
       " 'have you called her yet?',\n",
       " 'i do that whenever i have free time.',\n",
       " 'kids like to play.',\n",
       " \"you can't just leave me here.\",\n",
       " 'never tell a lie!',\n",
       " 'seemingly impossible things sometimes happen.',\n",
       " 'i am afraid of dying.',\n",
       " 'are you in boston now?',\n",
       " 'i ate while you were at the supermarket.',\n",
       " 'tom held his breath.',\n",
       " 'what can i do to convince you?',\n",
       " 'the boy entered by the back door.',\n",
       " 'you have my word.',\n",
       " \"i can't excuse her.\",\n",
       " 'put it back where you got it from.',\n",
       " \"what's wrong with those?\",\n",
       " 'i will only buy the car if they repair the brakes first.',\n",
       " 'he wants to be like me.',\n",
       " 'i polished up the floor and furniture.',\n",
       " 'i explained the rules of the game to them.',\n",
       " 'you look exactly like tom.',\n",
       " \"i didn't really like the stew that tom made.\",\n",
       " \"tom doesn't care about what mary says about him.\",\n",
       " 'my mother made me a sweater.',\n",
       " 'the hunter aimed at the bird, but missed.',\n",
       " 'i remember the house where i grew up.',\n",
       " 'i know the boy standing over there.',\n",
       " \"let's talk man to man.\",\n",
       " 'it seems like summer is finally here.',\n",
       " 'is tom out?',\n",
       " 'it was the general opinion.',\n",
       " 'how strange!',\n",
       " 'the fox and the bear lived together.',\n",
       " 'the whole world is watching.',\n",
       " 'find the cat.',\n",
       " 'what can you give me?',\n",
       " 'no one lived on the island at that time.',\n",
       " \"we're an hour behind.\",\n",
       " 'everything happens for a reason.',\n",
       " 'i feel like a drink.',\n",
       " \"i'm sorry i was so rude.\",\n",
       " \"i want to be sure you understand what's going to happen.\",\n",
       " \"food shouldn't be wasted.\",\n",
       " 'she is two years older than you.',\n",
       " \"don't come home too late.\",\n",
       " \"i've never played golf.\",\n",
       " \"tom isn't my boyfriend.\",\n",
       " 'tom gained weight over the winter.',\n",
       " \"i'm wet.\",\n",
       " 'this smells great! what are you cooking?',\n",
       " 'i plan to cycle around shikoku next year.',\n",
       " 'i think a movie is more entertaining than any book.',\n",
       " 'i want you to see her.',\n",
       " \"he's looking at me.\",\n",
       " \"do you have something that's good for a cough?\",\n",
       " \"i don't understand french.\",\n",
       " 'i want to hire tom.',\n",
       " \"how's your cold?\",\n",
       " \"we're sophomores.\",\n",
       " \"i'm looking for a sweater.\",\n",
       " \"i've got something to do.\",\n",
       " 'tom refused to open the door.',\n",
       " 'now try to sleep.',\n",
       " 'please fill this water bottle.',\n",
       " 'her skin is smooth.',\n",
       " 'his ambition is to be a lawyer.',\n",
       " 'they defended their country against the invaders.',\n",
       " 'she was in the hospital for six weeks because of her illness.',\n",
       " 'the baby smiled at me.',\n",
       " 'did you read it all?',\n",
       " 'i was in the shower.',\n",
       " 'i still dream about it.',\n",
       " 'were you at the theater last saturday?',\n",
       " \"for some reason the microphone didn't work earlier.\",\n",
       " \"i'd like to confide in you.\",\n",
       " \"here's a pen.\",\n",
       " 'tom used to work in a bakery.',\n",
       " 'cows are sacred to hindus.',\n",
       " 'his mother was right.',\n",
       " 'i asked for his help.',\n",
       " 'i wear glasses only for reading.',\n",
       " 'when was the last time it rained here?',\n",
       " 'she hates green peppers.',\n",
       " \"be careful. don't throw away those papers.\",\n",
       " \"i'll either write to you or phone you next week.\",\n",
       " 'tom forgave mary.',\n",
       " 'everything here is covered in dust.',\n",
       " \"i don't want to study french.\",\n",
       " \"he hurried up so that he wouldn't miss the train.\",\n",
       " \"i can't believe you actually did something like that.\",\n",
       " 'it was published in 1969.',\n",
       " 'when will you return?',\n",
       " 'that changed everything.',\n",
       " 'i went to the department store with the intent of buying a gift.',\n",
       " \"i'd like a little bit of help.\",\n",
       " 'i slept with my clothes on.',\n",
       " \"they say that she'll get married soon.\",\n",
       " 'keep your room as neat as you can.',\n",
       " \"everyone's shocked.\",\n",
       " 'what do you expect to happen?',\n",
       " 'the man answers to the description in the newspaper.',\n",
       " 'i went into the navy.',\n",
       " 'she married him when she was 20.',\n",
       " \"let's get the party started.\",\n",
       " 'i would never eat anything that tom makes.',\n",
       " 'he is at home.',\n",
       " 'what do you actually do in the office?',\n",
       " 'he crushed the sheet of paper up into a ball.',\n",
       " 'he leaned on his elbows.',\n",
       " \"isn't that the golden gate bridge?\",\n",
       " 'he learned the news while reading the newspaper.',\n",
       " 'it is a good camera.',\n",
       " 'the possibility that the explosion was caused by carelessness cannot be ruled out.',\n",
       " 'there used to be a large park here.',\n",
       " 'a detective arrived upon the scene of the crime.',\n",
       " \"tom didn't feel the earthquake.\",\n",
       " 'he made her a bookshelf.',\n",
       " 'let go of your hate.',\n",
       " 'tom and mary know what happened.',\n",
       " 'the country is rich in natural resources.',\n",
       " 'he will be coming to see me this afternoon.',\n",
       " 'tom was caught sneaking out of the room.',\n",
       " 'tom got angry when he found the door locked.',\n",
       " 'everyone liked tom.',\n",
       " \"tom's awake.\",\n",
       " \"i'll wait here until she comes.\",\n",
       " \"tom didn't have the courage to admit his mistake.\",\n",
       " 'tom loves his job.',\n",
       " 'tom needs to pay his own debts.',\n",
       " 'tom often feels drowsy after eating lunch.',\n",
       " 'they created a government.',\n",
       " \"we'll know soon.\",\n",
       " \"i've told you over and over again not to do that.\",\n",
       " \"i'm not sure if i like this one or that one.\",\n",
       " 'i have to get up early tomorrow. can you give me a call at six?',\n",
       " 'tom never said anything.',\n",
       " 'it is said that women live longer than men.',\n",
       " 'as soon as the dog saw me, it began to bark.',\n",
       " \"tom's girlfriend is canadian.\",\n",
       " 'now that it has stopped raining, we can go home.',\n",
       " 'how long do we stop here?',\n",
       " 'skiing is my passion.',\n",
       " 'i want to learn to sound more like a native speaker.',\n",
       " \"i didn't know you could juggle.\",\n",
       " \"let's start!\",\n",
       " 'i know a shorter route.',\n",
       " \"i'd like to reserve a single room.\",\n",
       " 'did anybody get hurt?',\n",
       " \"you can't buy anything if you have no money.\",\n",
       " 'love is like oxygen.',\n",
       " 'can you stand up?',\n",
       " 'he was hit by a car and died right away.',\n",
       " \"it's very difficult to understand her.\",\n",
       " 'of the 23 who were arrested, four escaped.',\n",
       " 'those are too big.',\n",
       " 'why should i worry about tom?',\n",
       " 'she made a man of him.',\n",
       " 'she always believes me.',\n",
       " 'hold still.',\n",
       " \"i can't stay for long.\",\n",
       " 'tom drives.',\n",
       " \"please don't mind me.\",\n",
       " \"why don't you let us go?\",\n",
       " \"i can't work like this.\",\n",
       " 'i think you should tell tom that.',\n",
       " \"you've given me your cold.\",\n",
       " \"he's just an amateur.\",\n",
       " \"i've just finished packing.\",\n",
       " 'they had good chemistry.',\n",
       " 'the movie is now showing at a theater near you.',\n",
       " 'good luck, tom.',\n",
       " \"i can't sleep with all this noise.\",\n",
       " 'i drank from the tap.',\n",
       " 'tom asked me if i liked chinese food.',\n",
       " 'they seem to have had a good time in rome.',\n",
       " 'what language do they speak in the united states?',\n",
       " 'i went twice.',\n",
       " 'i have a different idea.',\n",
       " 'he was killed with a sword.',\n",
       " \"i'm not working today.\",\n",
       " 'i went climbing in the alps.',\n",
       " 'nothing remained in the refrigerator.',\n",
       " 'tom has just finished writing a letter to mary.',\n",
       " 'oh, i got it.',\n",
       " 'i hurt my ankle.',\n",
       " 'this is neither new nor unknown.',\n",
       " 'i need to know what happened to tom.',\n",
       " 'may i use your car today?',\n",
       " 'we just wanted to win.',\n",
       " 'the teacher is sitting on the chair.',\n",
       " 'a child is playing harp.',\n",
       " \"if tom wins, i'll be happy.\",\n",
       " 'it might snow this evening.',\n",
       " 'i have a weird neighbor.',\n",
       " \"i've checked everywhere.\",\n",
       " 'he turned traitor.',\n",
       " \"you didn't call me.\",\n",
       " \"i don't trust you anymore.\",\n",
       " 'mt. fuji is the most famous mountain in japan.',\n",
       " \"tom was mary's only friend.\",\n",
       " 'tom helped mary get up from her seat.',\n",
       " 'you should watch your language when you talk to her.',\n",
       " 'do you remember how we met?',\n",
       " 'the kitten is taking a nap.',\n",
       " 'come to watch us play.',\n",
       " 'he thought that he was a genius.',\n",
       " 'i hope it helps.',\n",
       " 'i did it the old fashioned way.',\n",
       " 'my wife is suffering from pneumonia.',\n",
       " \"i still don't feel safe.\",\n",
       " \"i'll make some coffee.\",\n",
       " 'tom could understand how mary was feeling.',\n",
       " 'go brush your teeth.',\n",
       " 'he told me not to tell lies.',\n",
       " 'the ship lowered its gangway after docking.',\n",
       " \"don't make a mountain out of a molehill.\",\n",
       " \"i'll keep in touch.\",\n",
       " \"i hope i didn't wake you.\",\n",
       " \"i've drunk way too much coffee today.\",\n",
       " \"it's pretty incredible.\",\n",
       " \"i don't want you to panic.\",\n",
       " \"don't you think it's strange that he's not here?\",\n",
       " \"he said he wouldn't go to the party because he had to study.\",\n",
       " 'tom boarded the plane.',\n",
       " 'what else does tom need?',\n",
       " 'why are you so ugly?',\n",
       " 'they set fire to houses and farm buildings.',\n",
       " 'do you see a star?',\n",
       " 'do you live in tokyo?',\n",
       " \"shoes change one's life. just ask cinderella!\",\n",
       " 'i love you more than i love her.',\n",
       " 'i carried three books.',\n",
       " 'make your choice.',\n",
       " 'the students answered in order.',\n",
       " 'how long will you be staying here?',\n",
       " 'tom spoke calmly.',\n",
       " 'i said i would help you.',\n",
       " \"i don't feel well at such a high altitude.\",\n",
       " 'do you have any idea where tom put the keys?',\n",
       " 'he was born in the 19th century.',\n",
       " 'i am accustomed to living alone.',\n",
       " 'i had a good time, too.',\n",
       " \"tom doesn't make much money.\",\n",
       " 'she was in the americas last month.',\n",
       " \"i don't know how things were a hundred or fifty years ago.\",\n",
       " 'he should thank you.',\n",
       " 'we work from nine to five.',\n",
       " 'did you want to see me?',\n",
       " 'my parents met each other in the mountains.',\n",
       " 'i have to get some new clothes.',\n",
       " 'those are values that we all share.',\n",
       " \"i'm calling because my credit card has been stolen.\",\n",
       " 'we have nothing in common.',\n",
       " 'she knit him a sweater for his birthday.',\n",
       " \"don't be a fool.\",\n",
       " 'i like to study french.',\n",
       " 'variable names in c are case sensitive.',\n",
       " 'a unicycle has one wheel.',\n",
       " 'somebody must care for the patient.',\n",
       " 'tom is a wimp.',\n",
       " \"it's tom's idea.\",\n",
       " \"you shouldn't count on others for help.\",\n",
       " 'i thought tom would love the gift you gave him.',\n",
       " 'when was the last time you had dinner with your wife?',\n",
       " 'the accident resulted from his carelessness.',\n",
       " 'may i call on you some day?',\n",
       " 'her face turned white.',\n",
       " 'a rash broke out on her neck.',\n",
       " 'they lived in the countryside during the war.',\n",
       " 'compared with his brother, he is not so intelligent.',\n",
       " 'shake my hand.',\n",
       " 'this rule has no exceptions.',\n",
       " 'that copy differs from the original.',\n",
       " \"i didn't want to insult tom.\",\n",
       " 'next thursday is a holiday.',\n",
       " \"tom and i aren't identical twins.\",\n",
       " 'he lives in osaka.',\n",
       " 'fish is good for you.',\n",
       " 'they buried him in his grave.',\n",
       " \"no matter what we do, tom won't be satisfied.\",\n",
       " 'i made some corrections.',\n",
       " 'put down your pencil.',\n",
       " 'make it smaller.',\n",
       " \"unfortunately, we don't have what you want.\",\n",
       " 'i ran out of fuel.',\n",
       " \"this isn't my money.\",\n",
       " \"i'm a different person now.\",\n",
       " 'tom remained unmarried all his life.',\n",
       " 'oil may not last for another hundred years.',\n",
       " \"i'm very disappointed.\",\n",
       " 'i went through a lot of trouble.',\n",
       " \"you've got to stop doing that.\",\n",
       " 'i cut my little finger peeling potatoes.',\n",
       " 'tom said that he planned to stick around school all day.',\n",
       " 'you are big.',\n",
       " 'he was a good king.',\n",
       " 'i was scared of tom.',\n",
       " 'tom looks worried and confused.',\n",
       " 'how many people are at your party?',\n",
       " 'she was supposed to call him at 2:30.',\n",
       " 'she might come.',\n",
       " 'thousands of people there were arrested.',\n",
       " 'dancing is not a crime.',\n",
       " \"this fruit doesn't taste good.\",\n",
       " \"don't come near me. you're filthy.\",\n",
       " 'this tire needs some air.',\n",
       " 'the game is over.',\n",
       " 'save your breath.',\n",
       " \"i'm losing weight.\",\n",
       " \"tom's hardly said a word all day.\",\n",
       " \"i can't remember what i ate last night.\",\n",
       " 'i read the book up to page 80 yesterday.',\n",
       " 'even monkeys fall from trees.',\n",
       " 'tom bought some flowers for mary.',\n",
       " 'i think that he is right.',\n",
       " \"you don't want me to be late for school, do you?\",\n",
       " 'the law is the law.',\n",
       " 'you must be from australia.',\n",
       " \"i will be back in two week's time.\",\n",
       " 'the helicopter landed on the roof.',\n",
       " 'i want to become a journalist.',\n",
       " 'the zipper is stuck.',\n",
       " 'she speaks loudly.',\n",
       " 'we are so happy.',\n",
       " 'he came to my house on the pretext of seeing me.',\n",
       " 'when i entered the room, she was playing the piano.',\n",
       " 'tom wants me to fix this for him.',\n",
       " \"just don't tell anyone else.\",\n",
       " 'i do like fish.',\n",
       " 'nothing has changed.',\n",
       " 'would you please tell me the way to the station?',\n",
       " 'she is a very clever liar.',\n",
       " 'do you need an ambulance?',\n",
       " 'you have the right to consult a lawyer.',\n",
       " 'he is going to study english next week.',\n",
       " 'he likes tea.',\n",
       " \"you won't be the first.\",\n",
       " 'japan is a beautiful country.',\n",
       " 'do you think money will buy her happiness?',\n",
       " 'it was almost impossible to get around on that street.',\n",
       " \"it's snowing.\",\n",
       " 'he is doing penance.',\n",
       " 'has tom been informed?',\n",
       " \"you're impatient.\",\n",
       " 'you have to be very quiet.',\n",
       " 'the man sold his soul to the devil.',\n",
       " 'i see the boy.',\n",
       " \"i've always kind of liked it.\",\n",
       " 'you seem to know all the answers.',\n",
       " 'tom, mary and john sat around the kitchen table.',\n",
       " 'can you tell me what is happening?',\n",
       " 'i could never make him believe what i said.',\n",
       " 'my father asked me to open the window.',\n",
       " \"don't look at me that way.\",\n",
       " 'tom needs somebody to help him.',\n",
       " 'german is not only spoken in germany.',\n",
       " 'i got out of bed and had a good stretch.',\n",
       " 'the prime minister will hold a press conference tomorrow.',\n",
       " \"it seems impossible that you're that old.\",\n",
       " 'although i was tired, i did what i was able to do.',\n",
       " 'you will need a bodyguard.',\n",
       " 'what you are is more important than what you have.',\n",
       " 'there were two children playing on the street.',\n",
       " 'my little finger is swollen.',\n",
       " 'she wants to dye her hair red.',\n",
       " 'exercise is to the body what thinking is to the brain.',\n",
       " 'you may bring whoever wants to come.',\n",
       " \"i'm against the marriage.\",\n",
       " 'i forget your telephone number.',\n",
       " 'they took enough provisions for three years at sea.',\n",
       " 'i thought tom would say that.',\n",
       " 'i need to go get some eggs.',\n",
       " 'mary loves her bamboo fence.',\n",
       " 'does it work?',\n",
       " 'his words touched her heart.',\n",
       " \"tom didn't say that.\",\n",
       " 'spring has come.',\n",
       " 'put it there.',\n",
       " 'i could have done better if i had had more time.',\n",
       " \"it'd be better if you would come with me.\",\n",
       " 'i had a good reason for doing that.',\n",
       " 'everything seems to be in order.',\n",
       " 'would you sing us a song in french?',\n",
       " 'the old man saw my notebook and smiled at me.',\n",
       " 'tell me everything about it.',\n",
       " \"they're running late.\",\n",
       " 'my wife is wearing a blue dress.',\n",
       " 'will it bother you if i smoke?',\n",
       " \"i'm going to tell you something.\",\n",
       " \"let's not watch tv.\",\n",
       " 'are you aware that okinawa is closer to china than to honshu?',\n",
       " 'i disposed of my old coat.',\n",
       " 'he looked well.',\n",
       " 'bananas are delicious.',\n",
       " 'i forget your telephone number.',\n",
       " 'let the game begin.',\n",
       " \"something doesn't make sense here.\",\n",
       " 'my mother teaches flower arranging.',\n",
       " 'the police found some blood on the floor.',\n",
       " \"i don't know how to legally get around those regulations.\",\n",
       " 'i play football.',\n",
       " 'finish it before you go to bed.',\n",
       " 'it was like watching a slow motion movie.',\n",
       " \"i shouldn't have gone there by myself.\",\n",
       " \"you're my enemy.\",\n",
       " \"i don't feel like drinking vodka.\",\n",
       " 'i quit my job.',\n",
       " 'i found the box empty.',\n",
       " \"tom couldn't say anything publicly.\",\n",
       " \"i'd love to dance with you.\",\n",
       " 'tom asked me if i could go cycling with him on saturday.',\n",
       " 'there are no exceptions to this rule.',\n",
       " 'tom is kind to mary.',\n",
       " \"tell me why you think you can't trust me.\",\n",
       " 'i wish i could go to the concert.',\n",
       " 'she hit the ball hard.',\n",
       " 'today is the second of january.',\n",
       " 'do you know the words to that song?',\n",
       " 'give me that.',\n",
       " 'no one warned me.',\n",
       " \"please don't distract me from my work.\",\n",
       " 'i helped her hang the picture on the wall.',\n",
       " 'please take me to this address.',\n",
       " 'i arrived here yesterday.',\n",
       " \"tom didn't have on his shoes.\",\n",
       " 'tom and mary are teachers.',\n",
       " 'have some eggnog.',\n",
       " 'do you see me?',\n",
       " \"half of what tom says isn't true.\",\n",
       " 'payment is required in advance.',\n",
       " 'tom explained the circumstances to mary.',\n",
       " \"don't let anyone leave this building.\",\n",
       " 'he gave us an essay to write during the vacation.',\n",
       " 'we are engaged in a difficult task.',\n",
       " 'this new song is a big hit.',\n",
       " 'i took two aspirins for my headache.',\n",
       " 'a wood floor is beautiful.',\n",
       " 'how many people are here?',\n",
       " 'what does tom have to say?',\n",
       " 'that could solve a lot of problems.',\n",
       " \"this isn't my umbrella; it's somebody else's.\",\n",
       " 'columbus discovered america in 1492.',\n",
       " \"what's the minimum wage in australia?\",\n",
       " \"honesty doesn't pay.\",\n",
       " \"i've been requested to help you.\",\n",
       " 'any student can answer that question.',\n",
       " \"i didn't even know your name.\",\n",
       " \"those who choose not to read have no advantage over those who can't read.\",\n",
       " 'the cancer had spread to several organs.',\n",
       " \"don't be offended. tom's that way with everyone.\",\n",
       " 'it was magic.',\n",
       " 'have you given tom anything to eat or drink?',\n",
       " \"i can't afford a car.\",\n",
       " 'i had hoped to meet her there.',\n",
       " \"i'm still thirsty.\",\n",
       " \"what's your favorite small town in the united states?\",\n",
       " 'tom plans to stay here until it stops raining.',\n",
       " 'spain controlled florida.',\n",
       " 'the bride was radiant.',\n",
       " \"that doesn't bother me.\",\n",
       " \"you've probably done this many times before.\",\n",
       " \"you can't make an omelet without breaking eggs.\",\n",
       " 'the two families live under one roof.',\n",
       " 'recess ended.',\n",
       " 'there are many good reasons not to do it.',\n",
       " 'i obeyed the rules.',\n",
       " 'tom handed mary her jacket.',\n",
       " 'they are both working at the pet store.',\n",
       " \"don't judge a book by its cover.\",\n",
       " 'he works in the laboratory.',\n",
       " \"this summer we'll go to the mountains and to the sea.\",\n",
       " 'she married a musician.',\n",
       " 'they already knew.',\n",
       " 'do you have any idea what tom is talking about?',\n",
       " 'i have a friend who feeds his dog strawberries.',\n",
       " 'tom said he had to work on saturday.',\n",
       " 'tom wanted to eat cake and ice cream for dessert.',\n",
       " 'he told his mother that he would study the next day.',\n",
       " 'i joined him at the station.',\n",
       " 'tom came after mary left.',\n",
       " 'tom was caught in a lie.',\n",
       " 'i became very sleepy after a bit of reading.',\n",
       " 'tom is able to speak french.',\n",
       " \"it's possible that he came here when he was young.\",\n",
       " 'there are three survivors.',\n",
       " 'my friend will be our guide.',\n",
       " 'do you see the bird on the telephone wire?',\n",
       " 'tom helped mary set the table.',\n",
       " 'i had a terrible experience.',\n",
       " 'they had a pillow fight.',\n",
       " \"osaka is japan's second biggest city.\",\n",
       " \"tom couldn't accomplish everything he'd hoped he could.\",\n",
       " 'do you think so?',\n",
       " 'can the dentist see me today?',\n",
       " \"it wasn't hard.\",\n",
       " 'can you explain to me how i can get to the airport?',\n",
       " 'this is also my first time.',\n",
       " 'he made the plan along with his colleagues.',\n",
       " 'i have to change buses two times.',\n",
       " \"why isn't that working? it worked yesterday.\",\n",
       " 'how did tom find out?',\n",
       " \"this is why we can't have nice things.\",\n",
       " 'he will be playing tennis tomorrow afternoon.',\n",
       " 'tom waited to hear from mary.',\n",
       " 'she was made to wait for over an hour.',\n",
       " 'are you an only child?',\n",
       " 'do you like movies?',\n",
       " \"are there any religions that don't permit organ donation?\",\n",
       " 'could i borrow a hammer?',\n",
       " \"i won't tolerate your mistakes again.\",\n",
       " 'children need many things, but above all they need love.',\n",
       " \"it's not your style.\",\n",
       " 'i wrote to her last month.',\n",
       " 'why did you buy such an expensive dictionary?',\n",
       " \"i'll keep that book for myself.\",\n",
       " 'he trusted you.',\n",
       " 'nobody can make pizza as well as tom can.',\n",
       " 'do you want to go out with me tonight?',\n",
       " 'tom likes his coffee black.',\n",
       " 'please sit down.',\n",
       " 'this is very important meeting. you ought not to miss it.',\n",
       " \"tom had to take care of mary's son while she was working.\",\n",
       " 'my dream came true.',\n",
       " \"you'll thank me.\",\n",
       " 'some girls were playing tennis.',\n",
       " 'i awoke from a long dream.',\n",
       " \"i think you've made the wrong decision.\",\n",
       " 'nobody ever praises him.',\n",
       " \"i thought we'd be more comfortable here.\",\n",
       " 'i bowed politely.',\n",
       " \"that's what i should've said.\",\n",
       " 'my grandmother lived with us.',\n",
       " 'he was impatient to see his daughter.',\n",
       " 'he is far from rich.',\n",
       " 'he could not breathe deeply.',\n",
       " \"women don't drive as well as men.\",\n",
       " 'go out of the terminal and turn right.',\n",
       " 'you have one hour.',\n",
       " 'i solved the puzzle.',\n",
       " \"don't sit on the table. it could break.\",\n",
       " 'do you think you can handle a few minutes alone with tom?',\n",
       " 'they never stopped.',\n",
       " \"tom couldn't make it to school because of the typhoon.\",\n",
       " 'turn off the gas.',\n",
       " 'tom will never forgive you.',\n",
       " 'i ran out of the house.',\n",
       " 'is it right for a doctor to decide when someone should die?',\n",
       " \"mary can't decide whether or not to buy the dress.\",\n",
       " 'i wish i had married her.',\n",
       " 'tom misjudged mary.',\n",
       " \"you're not a prisoner here.\",\n",
       " 'tom often downloads movies.',\n",
       " 'you may choose whichever you want.',\n",
       " 'so what do you want with me?',\n",
       " 'keep a copy of that document.',\n",
       " \"she didn't mean to offend anyone with her remark.\",\n",
       " 'the cabinet resigned.',\n",
       " 'english is a compulsory subject.',\n",
       " 'do you have enough money?',\n",
       " 'i want you to be my friend again.',\n",
       " \"i'm just making sure.\",\n",
       " 'tom has something in his hand.',\n",
       " 'can you squeeze me in?',\n",
       " 'keep away from me.',\n",
       " 'i do not have a sister.',\n",
       " 'did you live here before?',\n",
       " 'jorge can speak four languages.',\n",
       " 'do you really want to wait for tom?',\n",
       " 'elephants are divided into three different species.',\n",
       " 'please bring the others.',\n",
       " 'he speaks fluently.',\n",
       " \"i don't want to tell you.\",\n",
       " 'the game is over.',\n",
       " 'you know everything.',\n",
       " 'cows give us milk.',\n",
       " \"i want to know about tom's new job.\",\n",
       " 'you came too late.',\n",
       " 'perhaps she will come tomorrow.',\n",
       " 'he is rich enough to buy two cars.',\n",
       " 'some of them have committed suicide.',\n",
       " 'do you know of any good restaurants around here?',\n",
       " \"this is tom's book.\",\n",
       " 'tom took the bus to school.',\n",
       " 'only members of the club are entitled to use this room.',\n",
       " 'tom is a refugee.',\n",
       " 'my father grew old.',\n",
       " 'cocoa can be very bitter.',\n",
       " 'i would never feed my dog commercial dog food.',\n",
       " 'tom loves horses.',\n",
       " 'tom assigned the job to mary.',\n",
       " 'i want that job.',\n",
       " 'write me an email if you feel like it.',\n",
       " 'she watched him draw a picture.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eng_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d907161",
   "metadata": {},
   "source": [
    "### 토큰화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4543972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab0f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././spa-eng_corpus.txt --model_prefix=spa-eng_spm --vocab_size=20000--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././spa-eng_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: spa-eng_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bo"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././spa-eng_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 236740 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=7880265\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9547% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=44\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999547\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 236740 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 110212 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 236740\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 63212\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 63212 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=35576 obj=10.7624 num_tokens=120312 num_tokens/piece=3.38183\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=28243 obj=8.53496 num_tokens=120792 num_tokens/piece=4.27688\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21997 obj=8.49074 num_tokens=127755 num_tokens/piece=5.80784\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21982 obj=8.47679 num_tokens=127812 num_tokens/piece=5.81439\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spa-eng_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spa-eng_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# 영어-스페인어 단어사전 공유\n",
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ab7677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentencepiece.SentencePieceProcessor; proxy of <Swig Object of type 'sentencepiece::SentencePieceProcessor *' at 0x7f4ed4c6a210> >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c76647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2d70c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92874cb8cba84431aac7eb77808f0114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edc9a7f951044a4b4d8970e7c6a2cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/118370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 동일 토크나이저로 토큰화\n",
    "eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n",
    "spa_corpus = make_corpus(train_spa_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccaf431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they arrived half an hour early.\n",
      "[1, 82, 853, 1509, 135, 1070, 666, 0, 2]\n",
      "\n",
      "\n",
      "se anticiparon media hora.\n",
      "[1, 33, 18255, 414, 929, 316, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 잘 되었나 확인\n",
    "print(train_eng_sentences[0])\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(train_spa_sentences[0])\n",
    "print(spa_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50eb4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 패딩 추가해 토큰 길이 50으로 설정\n",
    "MAX_LEN = 50\n",
    "enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n",
    "dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1b29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련에 사용되도록 영어와 스페인어 데이터를 묶어 배치 크기 텐서로 생성하기\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb709f18",
   "metadata": {},
   "source": [
    "## 번역 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0664ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 구현\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af68d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask  생성하기\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75570011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Head Attention 구현\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d82130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed Forward Network 구현\n",
    "\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a62c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder의 레이어 구현\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a3be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 레이어 구현\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6ca9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 구현\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5580e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 구현\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21d667a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 모듈 조립\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4ea5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "747d58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 구현\n",
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "823cde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate 인스턴스 선언 & Optimizer 구현\n",
    "\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48bbb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1655ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 정의\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6966da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38712301ef84abb9e811d093f73a157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbd1ca9b4344585ab051eb87809ac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b0d9f232ab4fde85601f95607c66f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "    \n",
    "    for (batch, (src, tgt)) in enumerate(train_dataset):\n",
    "        loss, _, _, _ = train_step(src, tgt, transformer, optimizer)\n",
    "        total_loss += loss\n",
    "        \n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_description(f'Epoch {epoch+1}, Loss: {total_loss / (batch+1):.4f}')\n",
    "    \n",
    "    tqdm_bar.close()\n",
    "\n",
    "    # 에폭마다 모델 저장 (필요에 따라)\n",
    "    # if (epoch + 1) % save_every == 0:\n",
    "    #     checkpoint.save(file_prefix=checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93076",
   "metadata": {},
   "source": [
    "### NLTK를 활용한 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb51fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:515: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# 아래 두 문장을 바꿔가며 테스트 해보세요\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933082b6",
   "metadata": {},
   "source": [
    "50점 이상인경우 진짜 훌륭한 번역\n",
    "보통 논문에서는 20~40점\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19167714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "source": [
    "#1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면 BLEU Score\n",
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486df97",
   "metadata": {},
   "source": [
    "0점에 가까운 BLEU Score가 나오는 원인을 알 수 있겠네요. 바로 3-gram와 4-gram에서 거의 0점을 받았기 때문인데요, 위 예시에서 번역문 문장 중 어느 3-gram도 원문의 3-gram과 일치하는 것이 없기 때문입니다. 2-gram이 0.18이 나오는 것은 원문의 11개 2-gram 중에 2개만이 번역문에서 재현되었기 때문입니다.\n",
    "\n",
    "하지만 만약 nltk의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.\n",
    "예전 버전에서는 위 수식에서 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸해버리기 때문에 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현되어 있었습니다. 하지만 1.0 은 모든 번역을 완벽히 재현했음을 의미하기 때문에 총점이 의도치 않게 높아질 수 있어요! 그럴 경우에는 BLEU Score가 바람직하지 못할 것(Undesirable) 이라는 경고문이 추가되긴 합니다.\n",
    "\n",
    "## SmoothingFunction()으로 BLEU Score 보정하기\n",
    "\n",
    "그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다. Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요.\n",
    "\n",
    "진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해 봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다.\n",
    "\n",
    "(참고) 각 method들의 상세한 설명은 nltk의 bleu_score 소스코드를 참고해 봅시다. sentence_bleu() 함수에 smoothing_function=None을 적용하면 method0가 기본 적용됨을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32519671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000004\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3b252",
   "metadata": {},
   "source": [
    "SmoothingFunction()로 BLEU score를 보정한 결과, 새로운 BLEU 점수는 무려, 5점으로 올라갔습니다. 거의 의미 없는 번역이라는 냉정한 평가를 받게 되는군요.😥\n",
    "\n",
    "여기서 BLEU-4가 BLEU-3보다 조금이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해 보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠.\n",
    "\n",
    "## 트랜스포머 모델의 번역 성능 알아보기\n",
    "\n",
    "위 예시를 조금만 응용하면 우리가 훈련한 모델이 얼마나 번역을 잘하는지 평가할 수 있습니다! 아까 0.5%의 데이터를 테스트셋으로 빼 둔 것을 기억하시죠? 테스트셋으로 모델의 BLEU Score를 측정하는 함수 eval_bleu() 를 구현해보도록 합시다!\n",
    "\n",
    "먼저 번역기가 문장을 생성하도록 translate() 함수를 정의하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7991236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n",
    "    padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=MAX_LEN,\n",
    "                                                           padding='post')\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)   \n",
    "    for i in range(MAX_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, \n",
    "                                      output,\n",
    "                                      enc_padding_mask,\n",
    "                                      combined_mask,\n",
    "                                      dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)  \n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)  \n",
    "    return result\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25cab3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# bleu 스코어 함수 작성\n",
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score\n",
    "        \n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09910aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  i had hoped that he would recover.\n",
      "Model Prediction:  ['me', 'pareció', 'que', 'él', 'se', 'iban', 'a', 'perder?']\n",
      "Real:  ['yo', 'esperaba', 'que', 'él', 'se', 'recuperaría.']\n",
      "Score: 0.137471\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1374708101760565"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한 문장에 대해 평가\n",
    "# Q. 인덱스를 바꿔가며 테스트해 보세요\n",
    "test_idx = 0\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 test_eng_sentences[test_idx], \n",
    "                 test_spa_sentences[test_idx], \n",
    "                 tokenizer, \n",
    "                 tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d7abd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전체 테스트 데이터에 대해 평가\n",
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    \n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56c05e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5859123456f44eb6b9438e531e1f4f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Sample: 594\n",
      "Total Score: 0.11040930272528456\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e1f0fe",
   "metadata": {},
   "source": [
    "## 번역 성능 측정하기 (2) Beam Search Decoder\n",
    "Greedy Decoding 대신 새로운 기법을 적용하면 우리 모델을 더 잘 평가할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07b93b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n",
    "\n",
    "    return sequences\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a154ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n",
      "커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n",
      "마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c8bb1",
   "metadata": {},
   "source": [
    "실 이 예시는 Beam Search를 설명하는 데에는 더없이 적당하지만 실제로 모델이 문장을 생성하는 과정과는 거리가 멉니다. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 prob_seq 처럼 확률을 정의할 수 없겠다는 생각이 머리를 스치죠. 각 단어에 대한 확률은 prob_seq 처럼 한 번에 정의가 되지 않고 이전 스텝까지의 단어에 따라서 결정되기 때문입니다!\n",
    "\n",
    "간단한 예시로, Beam Size가 2이고 Time-step이 2인 순간의 두 문장이 나는 밥을 , 나는 커피를 이라고 한다면 세 번째 단어로 먹는다 , 마신다 를 고려할 수 있습니다. 이때, 전자에서 마신다 에 할당하는 확률과 후자에서 마신다 에 할당하는 확률은 각각 이전 단어들인 나는 밥을 , 나는 커피를 에 따라서 결정되기 때문에 서로 독립적인 확률을 갖습니다. 예컨대 후자가 마신다 에 더 높은 확률을 할당할 것을 알 수 있죠! 위 소스에서처럼 \"3번째 단어는 항상 [마신다: 0.3, 먹는다:0.5, ...] 의 확률을 가진다!\" 라고는 할 수 없다는 겁니다.\n",
    "\n",
    "따라서 Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 합니다. Beam Size가 5라고 가정하면 맨 첫 단어로 적합한 5개의 단어를 생성하고, 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률) 를 가지고 있으니 각각의 순위를 매길 수 있겠죠? 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.\n",
    "\n",
    "위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 거구요! 다들 잘 이해하셨죠? 😃\n",
    "\n",
    "## Beam Search Decoder 작성 및 평가하기\n",
    "각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현하세요!\n",
    "\n",
    "편의에 따라서 두 기능을 하나의 함수에 구현해도 좋습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fc89337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# calc_prob() 구현\n",
    "def calc_prob(src_ids, tgt_ids, model):\n",
    "    # TODO: 코드 구현\n",
    "    return tf.math.softmax(predictions, axis=-1)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74dce72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_decoder() 구현\n",
    "def beam_search_decoder(sentence, \n",
    "                        src_len,\n",
    "                        tgt_len,\n",
    "                        model,\n",
    "                        src_tokenizer,\n",
    "                        tgt_tokenizer,\n",
    "                        beam_size):\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "    \n",
    "    src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                            maxlen=src_len,\n",
    "                                                            padding='post')\n",
    "\n",
    "    pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype=np.int64)\n",
    "    pred_tmp = np.zeros((beam_size, tgt_len), dtype=np.int64)\n",
    "\n",
    "    eos_flag = np.zeros((beam_size, ), dtype=np.int64)\n",
    "    scores = np.ones((beam_size, ))\n",
    "\n",
    "    pred_tmp[:, 0] = tgt_tokenizer.bos_id()\n",
    "\n",
    "    dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n",
    "    prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "    for seq_pos in range(1, tgt_len):\n",
    "        score_cache = np.ones((beam_size * beam_size, ))\n",
    "\n",
    "        # init\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n",
    "            pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n",
    "            pred_tmp[branch_idx, :seq_pos]\n",
    "\n",
    "        for branch_idx in range(beam_size):\n",
    "            cache_pos = branch_idx*beam_size\n",
    "\n",
    "            if seq_pos != 1:   # 모든 Branch를 로 시작하는 경우를 방지\n",
    "                dec_in = pred_cache[branch_idx, :seq_pos]\n",
    "                dec_in = tf.expand_dims(dec_in, 0)\n",
    "\n",
    "                prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n",
    "\n",
    "            for beam_idx in range(beam_size):\n",
    "                max_idx = np.argmax(prob)\n",
    "\n",
    "                score_cache[cache_pos+beam_idx] *= prob[max_idx]\n",
    "                pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n",
    "\n",
    "                prob[max_idx] = -1\n",
    "\n",
    "        for beam_idx in range(beam_size):\n",
    "            if eos_flag[beam_idx] == -1: continue\n",
    "\n",
    "            max_idx = np.argmax(score_cache)\n",
    "            prediction = pred_cache[max_idx, :seq_pos+1]\n",
    "\n",
    "            pred_tmp[beam_idx, :seq_pos+1] = prediction\n",
    "            scores[beam_idx] = score_cache[max_idx]\n",
    "            score_cache[max_idx] = -1\n",
    "\n",
    "            if prediction[-1] == tgt_tokenizer.eos_id():\n",
    "                eos_flag[beam_idx] = -1\n",
    "\n",
    "    pred = []\n",
    "    for long_pred in pred_tmp:\n",
    "        zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n",
    "        short_pred = long_pred[:zero_idx+1]\n",
    "        pred.append(short_pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b96368da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# blue\n",
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                            candidate,\n",
    "                            weights=weights,\n",
    "                            smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f23e71c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. beam_bleu() 함수를 구현해봅시다.\n",
    "def beam_bleu(reference, ids, tokenizer):\n",
    "    reference = reference.split()\n",
    "\n",
    "    total_score = 0.0\n",
    "    for _id in ids:\n",
    "        candidate = tokenizer.decode_ids(_id.tolist()).split()\n",
    "        score = calculate_bleu(reference, candidate)\n",
    "\n",
    "        print(\"Reference:\", reference)\n",
    "        print(\"Candidate:\", candidate)\n",
    "        print(\"BLEU:\", calculate_bleu(reference, candidate))\n",
    "\n",
    "        total_score += score\n",
    "        \n",
    "    return total_score / len(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14041f00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175/162916218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m beam_search_decoder(test_eng_sentences[test_idx],\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175/2156553781.py\u001b[0m in \u001b[0;36mbeam_search_decoder\u001b[0;34m(sentence, src_len, tgt_len, model, src_tokenizer, tgt_tokenizer, beam_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdec_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseq_pos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175/736605034.py\u001b[0m in \u001b[0;36mcalc_prob\u001b[0;34m(src_ids, tgt_ids, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# TODO: 코드 구현\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"슝=3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "test_idx = 1\n",
    "\n",
    "ids = \\\n",
    "beam_search_decoder(test_eng_sentences[test_idx],\n",
    "                    MAX_LEN,\n",
    "                    MAX_LEN,\n",
    "                    transformer,\n",
    "                    tokenizer,\n",
    "                    tokenizer,\n",
    "                    beam_size=5)\n",
    "\n",
    "bleu = beam_bleu(test_spa_sentences[test_idx], ids, tokenizer)\n",
    "print(bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1beafa",
   "metadata": {},
   "source": [
    " ## Data Augmentation, 그중에서도 Embedding을 활용한 Lexical Substitution\n",
    " \n",
    " gensim 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.\n",
    "\n",
    "직접 모델을 다운로드해 load 하는 방법\n",
    "gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법\n",
    "한국어는 gensim 에서 지원하지 않으므로 두 번째 방법을 사용할 수 없지만, 영어라면 얘기가 달라지죠! 아래 웹페이지의 Available data → Model 부분에서 공개된 모델의 종류를 확인할 수 있습니다.\n",
    "\n",
    "RaRe-Technologies/gensim-data\n",
    "대표적으로 사용되는 Embedding 모델은 word2vec-google-news-300 이지만 용량이 커서 다운로드에 많은 시간이 소요되므로 이번 실습엔 적합하지 않습니다. 우리는 적당한 사이즈의 모델인 glove-wiki-gigaword-300 을 사용할게요! 아래 소스를 실행해 사전 훈련된 Embedding 모델을 다운로드해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e44a8740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8b52457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bananas', 0.6691170930862427),\n",
       " ('mango', 0.5804104208946228),\n",
       " ('pineapple', 0.5492372512817383),\n",
       " ('coconut', 0.5462778806686401),\n",
       " ('papaya', 0.541056752204895),\n",
       " ('fruit', 0.52181077003479),\n",
       " ('growers', 0.4877638816833496),\n",
       " ('nut', 0.48399588465690613),\n",
       " ('peanut', 0.48062023520469666),\n",
       " ('potato', 0.48061180114746094)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ce31404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: you know ? all you need is attention .\n",
      "To: 'll know ? all you need is attention . \n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"you know ? all you need is attention .\"\n",
    "sample_tokens = sample_sentence.split()\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += wv.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "497d0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def lexical_sub(sentence, word2vec_model):\n",
    "    # 문장을 토큰으로 분할\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    result = \"\"\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word2vec_model:\n",
    "            # 선택된 토큰이 어휘 사전에 있는 경우\n",
    "            selected_token = token\n",
    "        else:\n",
    "            # 선택된 토큰이 어휘 사전에 없는 경우 그대로 사용\n",
    "            selected_token = None\n",
    "        \n",
    "        if selected_token is not None:\n",
    "            # 선택된 토큰을 가장 유사한 다른 토큰으로 교체\n",
    "            similar_words = word2vec_model.most_similar(selected_token)\n",
    "            if similar_words:\n",
    "                substitute = similar_words[0][0]  # 가장 유사한 단어 선택\n",
    "                result += substitute + \" \"\n",
    "            else:\n",
    "                result += token + \" \"  # 대체어가 없는 경우 원래 토큰 유지\n",
    "        else:\n",
    "            result += token + \" \"  # 선택된 토큰이 없는 경우 원래 토큰 유지\n",
    "    \n",
    "    return result.strip()  # 앞뒤 공백 제거 후 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corpus = []\n",
    "\n",
    "for old_src in tqdm(test_eng_sentences):\n",
    "    new_src = lexical_sub(old_src, wv)\n",
    "    if new_src is not None: \n",
    "        new_corpus.append(new_src)\n",
    "    # Augmentation이 없더라도 원본 문장을 포함시킵니다\n",
    "    new_corpus.append(old_src)\n",
    "\n",
    "print(new_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae394cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
