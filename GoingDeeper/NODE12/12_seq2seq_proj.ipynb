{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65063e8a",
   "metadata": {},
   "source": [
    "# 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17ad6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tarfile\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930311d",
   "metadata": {},
   "source": [
    "## step 1. 데이터 다운로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034b9414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['korean-english-park.train.ko', 'korean-english-park.train.en']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 압축 파일 경로\n",
    "compressed_file_path = \"korean-english-park.train.tar.gz\"\n",
    "\n",
    "# 압축 해제할 폴더 경로\n",
    "extracted_folder = \"data\" \n",
    "\n",
    "# 압축 파일 해제\n",
    "with tarfile.open(compressed_file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extracted_folder)\n",
    "\n",
    "# 내부 파일 목록 확인\n",
    "extracted_files = os.listdir(extracted_folder)\n",
    "print(\"Extracted files:\", extracted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c2d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터 형태 확인\n",
    "\n",
    "path_to_file = \"data/korean-english-park.train.ko\"\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    kr_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(kr_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in kr_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3428e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
      ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
      ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
      ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
     ]
    }
   ],
   "source": [
    "# 한국어 데이터 형태 확인\n",
    "\n",
    "path_to_file = \"data/korean-english-park.train.en\"\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    en_raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(kr_raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in en_raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843e305",
   "metadata": {},
   "source": [
    "## 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e22cbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841ac34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned list1: 77591\n",
      "Cleaned list2: 77591\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 중복을 제거하면서 동일한 인덱스의 데이터도 제거하기 위한 set 초기화\n",
    "seen_items = set()\n",
    "kr_corpus = []\n",
    "en_corpus = []\n",
    "\n",
    "# 중복 제거를 위한 반복문\n",
    "for item1, item2 in zip(kr_raw, en_raw):\n",
    "    if item1 not in seen_items:\n",
    "        kr_corpus.append(item1)\n",
    "        en_corpus.append(item2)\n",
    "        seen_items.add(item1)\n",
    "\n",
    "print(\"Cleaned list1:\", len(kr_corpus))\n",
    "print(\"Cleaned list2:\", len(en_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ab738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence, lang, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    if lang == 'en':\n",
    "        sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "        sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "        sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    elif lang == 'ko':\n",
    "        mecab = Mecab()\n",
    "        sentence = ' '.join(mecab.morphs(sentence))\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # decoder의 입력 문장과 라벨로 사용할 출력 문장에 꼭 필요\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "632ad0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_corpus_cleaned = []\n",
    "en_corpus_cleaned = []\n",
    "\n",
    "for i in kr_corpus:\n",
    "    kr_corpus_cleaned.append(preprocess_sentence(i, lang='kr', s_token=False, e_token=False))\n",
    "    \n",
    "for i in en_corpus:    \n",
    "    en_corpus_cleaned.append(preprocess_sentence(i, lang='en', s_token=True, e_token=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817479b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\n",
      "<start> so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable , wireless mouse . <end>\n"
     ]
    }
   ],
   "source": [
    "print(kr_corpus_cleaned[1])\n",
    "print(en_corpus_cleaned[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d69e67a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of kr_corpus_filterted: 14848\n",
      "len of en_corpus_filterted: 14848\n"
     ]
    }
   ],
   "source": [
    "# 토큰 길이가 40 이하인 데이터 선별\n",
    "max_length = 40\n",
    "en_corpus_filtered = []\n",
    "kr_corpus_filtered = []\n",
    "\n",
    "for item1, item2 in zip(kr_corpus_cleaned, en_corpus_cleaned):\n",
    "    if len(item1) <= max_length:\n",
    "        kr_corpus_filtered.append(item1)\n",
    "        en_corpus_filtered.append(item2)\n",
    "\n",
    "print(\"len of kr_corpus_filterted:\", len(kr_corpus_filtered))\n",
    "print(\"len of en_corpus_filterted:\", len(en_corpus_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7803429d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"', '그러나 이것은 또한 책상도 필요로 하지 않는다.', '많은 인질들이 화학 가스의 영향으로 고통을 겪으며 병원으로 옮겨졌다.']\n",
      "['<start> much of personal computing is about can you top this ? <end>', '<start> like all optical mice , but it also doesn t need a desk . <end>', '<start> many captives were taken to hospital suffering from the effects of the chemical . <end>']\n"
     ]
    }
   ],
   "source": [
    "print(kr_corpus_filtered[:3])\n",
    "print(en_corpus_filtered[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf5ae8",
   "metadata": {},
   "source": [
    "## step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2cd2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, lang):\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre')\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bb75b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "kr_token, kr_tokenizer = tokenize(kr_corpus_filtered, lang='kr')\n",
    "en_token, en_tokenizer = tokenize(en_corpus_filtered, lang='en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b51588bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,  4146,   296,  6358,  6359,  1911, 11823,\n",
       "        11824,     6, 11825],\n",
       "       [    0,     0,     0,     0,     0,     5,   240,    19, 11826,\n",
       "         2977,   122,   110],\n",
       "       [    0,     0,     0,    34,  4147,  6360, 11827,  6361,  2341,\n",
       "        11828,   329,  2978],\n",
       "       [    0,     0,     0,     0,     0,     0, 11829,   932, 11830,\n",
       "         2979,  2342,  1912],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,  6362,   599]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr_token[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda382e",
   "metadata": {},
   "source": [
    "## Step 4. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "820ff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "075062b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        # TODO: Awesome Encoder Modules\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # TODO: Awesome Process\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dce21902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d201cdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 16439)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(kr_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(en_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f089c2a",
   "metadata": {},
   "source": [
    "## Step 5. 훈련하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1db35574",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy( # 모델이 생성한 확률 분포와 정수 인덱스 비교해 cross entropy 계산\n",
    "    from_logits=True, reduction='none') # from_logits=True: softmax 거치지 않고 모델 출력 그대로 전달\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype) # 마스킹 통해 패딩 많은 문제 해결\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "705ca88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # 훈련 외적 연산을 gpu에서 동작시켜 훈련 가속\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape: #모든 미분 연산 기록\n",
    "        enc_out = encoder(src) # context vector 생성\n",
    "        h_dec = enc_out[:, -1] # t=0일때 final hidden state\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "        \n",
    "        # <start> 문장과 enc_out, hiddenstate를 기반으로 다음 단어 예측\n",
    "        # 예측 단어와 정답간 loss 구한 뒤 t=1의 정답 단어를 다음 입력으로 사용\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2442bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 232/232 [03:17<00:00,  1.18it/s, Loss 1.6668]\n",
      "Epoch  2: 100%|██████████| 232/232 [02:02<00:00,  1.89it/s, Loss 1.6229]\n",
      "Epoch  3: 100%|██████████| 232/232 [02:02<00:00,  1.89it/s, Loss 1.6282]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "be_total_loss = 99999\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kr_token.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kr_token[idx:idx+BATCH_SIZE],\n",
    "                                en_token[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                en_tokenizer)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    \n",
    "    if be_total_loss < (total_loss / (batch + 1)):\n",
    "        break\n",
    "    else:\n",
    "        be_total_loss = (total_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d0f58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 저장\n",
    "encoder.save_weights('./encoder_weights_50000_ep1')\n",
    "decoder.save_weights('./decoder_weights_50000_ep1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "315a4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eval(txt):\n",
    "    sentence = txt.strip()                                         # 문장의 양쪽 공백 제거\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)                   # 특수 문자 및 구두점 주변에 공백 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                         # 여러 개의 공백을 하나의 공백으로 대체\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", sentence)  # 한글 및 영어 이외의 문자는 공백으로 대체\n",
    "    sentence = sentence.strip()                                         # 다시 양쪽 공백 제거\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02cc793c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 오바마는 대통령이다 .\n",
      "Output: <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> \n",
      "Input: 시민들은 도시 속에 산다 .\n",
      "Output: <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> \n",
      "Input: 커피는 필요 없다 .\n",
      "Output: <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> \n",
      "Input: 일곱 명의 사망자가 발생했다 .\n",
      "Output: <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> \n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((en_token.shape[-1], kr_token.shape[-1]))\n",
    "    \n",
    "    sentence = preprocessing_eval(sentence)\n",
    "    inputs = kr_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=kr_token.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([en_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(en_token.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += en_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if en_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Output: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    #plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "'''\n",
    "K1) 오바마는 대통령이다.\n",
    "K2) 시민들은 도시 속에 산다.\n",
    "K3) 커피는 필요 없다.\n",
    "K4) 일곱 명의 사망자가 발생했다.\n",
    "'''\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)\n",
    "translate(\"시민들은 도시 속에 산다.\", encoder, decoder)\n",
    "translate(\"커피는 필요 없다.\", encoder, decoder)\n",
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078afe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
